# HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs
<div align="center">
<h3>
  HuatuoGPT-o1
</h3>
</div>

<p align="center">
ğŸ“ƒ <a href="https://arxiv.org/pdf/2412.18925" target="_blank">Paper</a> ï½œğŸ¤— <a href="https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-7B" target="_blank">HuatuoGPT-o1-7B</a> ï½œğŸ¤— <a href="https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-8B" target="_blank">HuatuoGPT-o1-8B</a> ï½œ ğŸ¤— <a href="https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-70B" target="_blank">HuatuoGPT-o1-70B</a>  | ğŸ“š <a href="https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT" target="_blank">Data</a>
</p>


## âš¡ Introduction
Hello! Welcome to the repository for [HuatuoGPT-o1](https://arxiv.org/pdf/2412.18925)!

<div align=center>
<img src="assets/pic1.jpg"  width = "90%" alt="HuatuoGPT-o1" align=center/>
</div>


**HuatuoGPT-o1** is a medical LLM designed for advanced medical reasoning. It can identify mistakes, explore alternative strategies, and refine its answers.  By leveraging verifiable medical problems and a specialized medical verifier, it advances reasoning through:

- Using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs.
- Applying reinforcement learning (PPO) with verifier-based rewards to enhance complex reasoning further.

We open-sourced our models, data, and code here.

## ğŸ‘¨â€âš•ï¸ Model
- **Model Access**

|                      | Backbone     | Supported Languages | Link                                                                  |
| -------------------- | ------------ | ----- | --------------------------------------------------------------------- |
| **HuatuoGPT-o1-8B**  | LLaMA-3.1-8B  | English    | [HF Link](https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-8B) |
| **HuatuoGPT-o1-70B** | LLaMA-3.1-70B | English    | [HF Link](https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-70B) |
| **HuatuoGPT-o1-7B**  | Qwen2.5-7B   | English & Chinese | [HF Link](https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-7B) |
| **HuatuoGPT-o1-72B** | Qwen2.5-72B  | English & Chinese | [HF Link](https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-72B) |

- **Deploy**

HuatuoGPT-o1 can be used just like `Llama-3.1-8B-Instruct`. You can deploy it with tools like [vllm](https://github.com/vllm-project/vllm) or [Sglang](https://github.com/sgl-project/sglang),  or perform direct inference:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("FreedomIntelligence/HuatuoGPT-o1-8B",torch_dtype="auto",device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("FreedomIntelligence/HuatuoGPT-o1-8B")

input_text = "How to stop a cough?"
messages = [{"role": "user", "content": input_text}]

inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True
), return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

HuatuoGPT-o1 adopts a *thinks-before-it-answers* approach, with outputs formatted as:

```
## Thinking
[Reasoning process]

## Final Response
[Output]
```

## ğŸ“š Data
- **Data Access**

| Data                  | Description                                                                                   | Link                                                                                           |
| -------------------------- | ----------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| Medical Verifiable Problems | Open-ended medical problems sourced from challenging medical exams,  paired with ground-truth answers. | [Link](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-verifiable-problem)  |
| SFT Data in Stage 1        | Fine-tuning data generated using GPT-4o, including complex chains of thought (**Complex CoT**) and output (**Response**). | [Link](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)       |

- **Data Construction**

We provide scripts to construct verifiable problems and searching reasoning paths.

**1. Constructing Verifiable Problems from Multi-choice Questions.** 
```bash
python construct_verifiable_medical_problems.py --data_path  data/demo_data.json --filter_data --model_name gpt-4o --api_key [your api key]
```
**2. Searching Complex Reasoning Paths for SFT**

```bash
python search_for_complex_reasoning_path.py --data_path  data/demo_data.json --efficient_search True  --max_search_attempts 1 --max_search_depth 2 --model_name gpt-4o --api_key [your api key]
```


## ğŸš€ Training

- **Stage 1: Supervised Fine-Tuning (SFT)**

Fine-tune the model on an 8-GPU setup:
```bash
accelerate launch --config_file ./configs/deepspeed_zero3.yaml \
    --num_processes 8  \
    --num_machines 1 \
    --machine_rank 0 \
    --deepspeed_multinode_launcher standard SFT_stage1.py \
    --model_path [meta-llama/Llama-3.1-8B-Instruct] \
    --data_path [FreedomIntelligence/medical-o1-reasoning-SFT] 
```

- **Stage 2: Reinforcement Learning (RL)**

We provide a simple PPO script using the [trl](https://github.com/huggingface/trl) library. Below is an example for training an 8B model with PPO on an 8-GPU A100 machine. Ensure you first download our [medical verifier](https://huggingface.co/FreedomIntelligence/medical_o1_verifier_3B) as the reward model.

```bash
accelerate launch \
	--num_processes 8 \
	--num_machines 1 \
	--machine_rank 0 \
    --config_file ./configs/deepspeed_zero3.yaml \
	--deepspeed_multinode_launcher standard RL_stage2.py \
    --model_name_or_path [FreedomIntelligence/HuatuoGPT-o1-8B] \
    --reward_model_path [FreedomIntelligence/medical_o1_verifier_3B] \
    --value_model_path [meta-llama/Llama-3.2-3B-Instruct] \
    --dataset_name  [FreedomIntelligence/medical-o1-verifiable-problem]\
    --response_length 1300 \
    --temperature 0.5 \
    --local_rollout_forward_batch_size 8 \
    --num_ppo_epochs 3 \
    --num_mini_batches 1 \
    --total_episodes 20000 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --bf16 True \
    --output_dir ./ckpts \
    --save_strategy steps \
    --save_step 20 \
    --save_total_limit 1 \
    --eval_strategy steps \
    --eval_steps 20 \
    --kl_coef 0.03 \
    --learning_rate 5e-7 \
    --warmup_ratio 0.05 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --run_name ppo_medical_o1_8B \
    --num_sample_generations -1 \
    --report_to wandb
```

## ğŸ§ Evaluation
1. You first need to install [Sglang](https://github.com/sgl-project/sglang). After installation, deploy the model you want to test using Sglang with the following command:
```bash
log_num=0
model_name="FreedomIntelligence/HuatuoGPT-o1-8B" # Path to the model you are deploying
port=28${log_num}35
CUDA_VISIBLE_DEVICES=0  python -m sglang.launch_server --model-path $model_name --port $port --mem-fraction-static 0.8 --dp 1 --tp 1  > sglang${log_num}.log 2>&1 &
```
```Plain Text
1. `log_num=0`:
   -  å®šä¹‰ä¸€ä¸ªå˜é‡ `log_num` å¹¶å°†å…¶èµ‹å€¼ä¸º 0ã€‚è¿™ä¸ªå˜é‡å°†åœ¨åç»­çš„å‘½ä»¤ä¸­ä½¿ç”¨ï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶åå’Œç«¯å£å·ï¼Œæ–¹ä¾¿ç®¡ç†å¤šä¸ª sglang æœåŠ¡å®ä¾‹ã€‚

2. `model_name="FreedomIntelligence/HuatuoGPT-o1-8B"`:
   -  å®šä¹‰ä¸€ä¸ªå˜é‡ `model_name`ï¼Œå¹¶å°†å…¶èµ‹å€¼ä¸º `"FreedomIntelligence/HuatuoGPT-o1-8B"`ã€‚è¿™é€šå¸¸ä»£è¡¨æ‰˜ç®¡åœ¨ Hugging Face Hub ä¸Šçš„ä¸€ä¸ªæ¨¡å‹ ID æˆ–æœ¬åœ°æ¨¡å‹çš„æ–‡ä»¶è·¯å¾„ã€‚`FreedomIntelligence/HuatuoGPT-o1-8B`  å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªç‰¹å®šç‰ˆæœ¬çš„æˆ–å¾®è°ƒè¿‡çš„ LLM åç§°ã€‚ sglang ä¼šåŠ è½½å¹¶ä½¿ç”¨è¿™ä¸ªæŒ‡å®šçš„æ¨¡å‹ã€‚

3. `port=28${log_num}35`:
   -  å®šä¹‰ä¸€ä¸ªå˜é‡ `port`ï¼Œå¹¶å°†å…¶èµ‹å€¼ä¸ºä¸€ä¸ªç”±å­—ç¬¦ä¸²å’Œå˜é‡ç»„åˆæˆçš„æ•°å€¼ã€‚`${log_num}` ä¼šè¢«æ›¿æ¢ä¸º `log_num` å˜é‡çš„å€¼(0)ï¼Œæ‰€ä»¥æœ€ç»ˆ `port` çš„å€¼ä¸º `28035`ã€‚ è¿™ä¸ªç«¯å£å·å°†ç”¨äº sglang æœåŠ¡çš„ç›‘å¬ï¼Œå®¢æˆ·ç«¯å¯ä»¥é€šè¿‡è¿™ä¸ªç«¯å£ä¸æœåŠ¡è¿›è¡Œé€šä¿¡ã€‚ä½¿ç”¨å˜é‡å¯ä»¥è®©å¤šä¸ªæœåŠ¡å®ä¾‹è¿è¡Œåœ¨ä¸åŒçš„ç«¯å£ä¸Šï¼Œé¿å…å†²çªã€‚

4. `CUDA_VISIBLE_DEVICES=0`:
   -  è¿™æ˜¯ä¸€ä¸ªç¯å¢ƒå˜é‡è®¾ç½®ã€‚`CUDA_VISIBLE_DEVICES=0`  å‘Šè¯‰ CUDA åªä½¿ç”¨ GPU è®¾å¤‡ 0ã€‚ å¦‚æœç³»ç»Ÿæœ‰å¤šä¸ª GPUï¼Œè¿™ä¸ªè®¾ç½®å¯ä»¥æ§åˆ¶ sglang æœåŠ¡ä½¿ç”¨å“ªä¸ª GPUã€‚è¿™å¯¹äºå¤š GPU æœåŠ¡å™¨éå¸¸é‡è¦ï¼Œå¯ä»¥è¿›è¡Œèµ„æºåˆ†é…å’Œéš”ç¦»ã€‚

5. `python -m sglang.launch_server`:
   -  è°ƒç”¨ Python è§£é‡Šå™¨æ¥æ‰§è¡Œ `sglang.launch_server` æ¨¡å—ã€‚ `-m`  é€‰é¡¹å‘Šè¯‰ Python å°† `sglang.launch_server`  è§†ä¸ºä¸€ä¸ªæ¨¡å—æ¥è¿è¡Œã€‚ `sglang.launch_server`  å¾ˆå¯èƒ½æ˜¯ sglang åº“æä¾›çš„ç”¨äºå¯åŠ¨æœåŠ¡çš„è„šæœ¬ã€‚

6. `--model-path $model_name`:
   -  è¿™æ˜¯ä¸€ä¸ªä¼ é€’ç»™ `sglang.launch_server`  çš„å‘½ä»¤è¡Œå‚æ•°ã€‚`--model-path`  æŒ‡å®šäº†è¦åŠ è½½çš„æ¨¡å‹çš„è·¯å¾„æˆ– IDã€‚`${model_name}` ä¼šè¢«æ›¿æ¢ä¸ºä¹‹å‰å®šä¹‰çš„ `model_name`  å˜é‡çš„å€¼ã€‚

7. `--port $port`:
   -  å¦ä¸€ä¸ªä¼ é€’ç»™ `sglang.launch_server`  çš„å‘½ä»¤è¡Œå‚æ•°ã€‚`--port`  æŒ‡å®šäº†æœåŠ¡ç›‘å¬çš„ç«¯å£ã€‚`${port}` ä¼šè¢«æ›¿æ¢ä¸ºä¹‹å‰å®šä¹‰çš„ `port`  å˜é‡çš„å€¼ã€‚

8. `--mem-fraction-static 0.8`:
   -  æŒ‡å®šé™æ€å†…å­˜åˆ†é…çš„æ¯”ä¾‹ã€‚è¿™å‘Šè¯‰ sglang æœåŠ¡é¢„ç•™ 80% çš„ GPU å†…å­˜ï¼Œä»¥ä¾›æ¨¡å‹å’Œç›¸å…³æ“ä½œä½¿ç”¨ã€‚ é™æ€åˆ†é…å¯ä»¥æé«˜æ€§èƒ½ï¼Œé¿å…åŠ¨æ€åˆ†é…å¸¦æ¥çš„å¼€é”€ã€‚

9. `--dp 1 --tp 1`:
   - è¿™äº›å‚æ•°æ§åˆ¶æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰å’Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„è®¾ç½®ã€‚ `--dp 1`  å’Œ `--tp 1`  éƒ½è®¾ç½®ä¸º 1 æ„å‘³ç€ç¦ç”¨æ•°æ®å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œã€‚  å½“è®¾ç½®ä¸º1æ—¶ï¼Œè¡¨ç¤ºæ‰€æœ‰æ•°æ®å’Œå¼ é‡éƒ½å­˜å‚¨åœ¨ä¸€ä¸ªè®¾å¤‡ä¸Šã€‚ å¦‚æœæ‚¨æœ‰å¤šä¸ª GPU å¹¶ä¸”æƒ³è¦åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œæ‚¨å¯ä»¥å¢åŠ è¿™äº›å€¼ä»¥å¯ç”¨å¹¶è¡Œå¤„ç†ã€‚

10. `> sglang${log_num}.log 2>&1 &`:
    -  è¿™ä¸€éƒ¨åˆ†å¤„ç†è¾“å‡ºé‡å®šå‘å’Œåå°è¿è¡Œã€‚
    -  `>`  ç¬¦å·å°†æ ‡å‡†è¾“å‡º (stdout) é‡å®šå‘åˆ°åä¸º `sglang${log_num}.log`  çš„æ–‡ä»¶ã€‚ç”±äº `log_num` ä¸º 0ï¼Œæ–‡ä»¶åå°†æ˜¯ `sglang0.log`ã€‚
    -  `2>&1`  å°†æ ‡å‡†é”™è¯¯ (stderr) é‡å®šå‘åˆ°ä¸æ ‡å‡†è¾“å‡ºç›¸åŒçš„ä½ç½®ã€‚è¿™æ„å‘³ç€é”™è¯¯ä¿¡æ¯ä¹Ÿä¼šè¢«å†™å…¥åˆ° `sglang0.log`  æ–‡ä»¶ä¸­ã€‚
    -  `&`  ç¬¦å·å°†æ•´ä¸ªå‘½ä»¤æ”¾åœ¨åå°è¿è¡Œã€‚ è¿™æ„å‘³ç€è¯¥å‘½ä»¤å°†åœ¨åå°å¯åŠ¨ï¼Œè€Œä¸ä¼šé˜»å¡å½“å‰çš„ç»ˆç«¯ä¼šè¯ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œè¿™æ¡å‘½ä»¤å¯åŠ¨äº†ä¸€ä¸ª sglang æœåŠ¡ï¼ŒåŠ è½½æŒ‡å®šçš„ LLM (HuatuoGPT-o1-8B)ï¼Œç›‘å¬åœ¨ç«¯å£ 28035ï¼Œ ä½¿ç”¨ GPU 0ï¼Œè®¾ç½®é™æ€å†…å­˜åˆ†é…ï¼Œç¦ç”¨å¹¶è¡Œå¤„ç†ï¼Œå¹¶å°†æœåŠ¡çš„è¾“å‡ºå’Œé”™è¯¯ä¿¡æ¯å†™å…¥ `sglang0.log`  æ–‡ä»¶ï¼Œå¹¶ä¸”åœ¨åå°è¿è¡Œè¯¥æœåŠ¡ã€‚
```


2. Wait for the model to be deployed. After deployment, you can run the following code for evaluation. We use prompts that allow the model to respond freely. We find that the extracted results are consistently reliable and broadly cover the intended scope. You can also set the `--strict_prompt` option to use stricter prompts for more precise answer extraction.
```bash
python evaluation/eval.py --model_name $model_name  --eval_file evaluation/data/eval_data.json --port $port 
```
3. After completing the evaluation, run the following code to stop the Sglang service and release GPU memory.
```bash
bash evaluation/kill_sglang_server.sh
```
The evaluation code above can be used to test most models supported by Sglang.

## ğŸ©º HuatuoGPT Series 

Explore our HuatuoGPT series:
- [**HuatuoGPT**](https://github.com/FreedomIntelligence/HuatuoGPT): Taming Language Models to Be a Doctor
- [**HuatuoGPT-II**](https://github.com/FreedomIntelligence/HuatuoGPT-II): One-stage Training for Medical Adaptation of LLMs
- [**HuatuoGPT-Vision**](https://github.com/FreedomIntelligence/HuatuoGPT-Vision): Injecting Medical Visual Knowledge into Multimodal LLMs at Scale
- [**CoD (Chain-of-Diagnosis)**](https://github.com/FreedomIntelligence/Chain-of-Diagnosis): Towards an Interpretable Medical Agent using Chain of Diagnosis
- [**HuatuoGPT-o1**](https://github.com/FreedomIntelligence/HuatuoGPT-o1): Towards Medical Complex Reasoning with LLMs


## ğŸ“– Citation
```
@misc{chen2024huatuogpto1medicalcomplexreasoning,
      title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs}, 
      author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
      year={2024},
      eprint={2412.18925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18925}, 
}
```


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT-o1&type=Date)](https://star-history.com/#FreedomIntelligence/HuatuoGPT-o1&Date)
